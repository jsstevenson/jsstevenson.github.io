<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Preference and restriction bias in decision trees | James S. Stevenson </title> <meta name="author" content="James S. Stevenson "> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%87&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jsstevenson.github.io/blog/2021/dt-bias/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">James S. Stevenson </span> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">research </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Preference and restriction bias in decision trees</h1> <p class="post-meta"> September 04, 2021 </p> <p class="post-tags"> <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a>   ·   <a href="/blog/category/data-science"> <i class="fa-solid fa-tag fa-sm"></i> data science</a>   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine learning</a>   <a href="/blog/category/grad-school"> <i class="fa-solid fa-tag fa-sm"></i> grad school</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In machine learning, we begin with the space of all possible hypotheses about the true function which best predicts the data. These could be linear functions, polynomials, series of conjunctions and disjunctions, and so on. The model that we use to select from this hypothesis space necessarily imposes inductive biases, about how to generalize to the function from the training data. This is obviously a necessary step – it is infeasible to manually evaluate all possible function approximations – as much as it is a risky one.</p> <p>Mitchell (1997) distinguishes between two types of inductive bias: <em>preference</em> and <em>restriction</em>. <em>Restriction bias</em> narrows the hypothesis space by eliminating all but certain kinds of functions. A single-layer perceptron, for example, famously can only produce linearly separable classifications, constraining it from modeling even simple functions like XOR. <em>Preference bias</em>, in contrast, refers to how a single hypothesis might be selected from the space of all possible hypotheses. Linear regression will prefer those that minimize a loss function. Decision trees will typically prefer shorter (‘simpler’) trees over longer ones. KNN prefers… nearer neighbors over farther ones, per some definition of ‘near’. And so on.</p> <p>In other words, it looks like restriction bias limits what kinds of functions we can get as hypotheses, and preference bias tells us how we pick a favorite. We might even be tempted to think about it like the restriction tells us the general equation (like y = mx + b), and the preference tells us the specific values for the equation (<code class="language-plaintext highlighter-rouge">m = 3</code>, <code class="language-plaintext highlighter-rouge">b = 2.5</code>). But isn’t a preference metric (eg a particular loss function) a limit - maybe even a restriction - on ‘bad’ hypotheses?</p> <p>My thought here was that the least-leaky way to think about this is that restrictions describe the space of possible hypothesis functions before we ever see any data, and preferences describe narrowing that space once we’ve seen data. So, for example, decision trees (without any other tricks) prefer shorter trees, but for any tree of depth <code class="language-plaintext highlighter-rouge">k</code>, you could envision a dataset that would produce a tree of depth <code class="language-plaintext highlighter-rouge">k + 1</code> with the same algorithm. It’s true that you could never, with a DT, yield a tree where deeper nodes carry more information gain than shallower nodes, but that only becomes true after we’ve seen data.</p> <p>Pre-pruning, where you pre-register a max tree depth <code class="language-plaintext highlighter-rouge">k</code>, does impose a hard restriction, because you could never discover/produce a dataset that yields from that model a tree of depth <code class="language-plaintext highlighter-rouge">n + 1</code>. Post-pruning, where you (for example) select some subtree where a penalty term a counterweights against excessive numbers of leafs, doesn’t render any particular hypothesis unreachable – even if a given training set ends up chopping off a few leafs, you could probably change nothing but the targets to make it so those chops aren’t worth the cost (let alone envision a different training set where the pruned result looks like the tree from the first example before any post-pruning is applied).</p> <p>However, Mitchell says the ‘candidate elimination’ algorithm imposes a restriction bias by only considering functions that are consistent with the data. This, of course, suggests that our ‘data awareness’ heuristic is incorrect. We might be tempted to say that the candidate elimination algorithm assumes data consistency, so it cannot produce a hypothesis when data contradicts – for example, if we have training data <code class="language-plaintext highlighter-rouge">((2, 3), (2, 1))</code>, it would simply produce no solution, but a decision tree (depending on the flavor) might settle for a deciding factor of <code class="language-plaintext highlighter-rouge">m = 2</code>. However, that’s stil introducing awareness of the data – before that, there’s nothing that prevents the candidate elimination algorithm from yielding that same hypothesis.</p> <p>Isbell ultimately rules that “[m]aking it impossible to have a tree greater than depth k is a restriction bias. Making it less likely is a preference bias.” I think I’m pretty capable of applying this rule, but I’m not sure if I’ve successfully absorbed its logic, though.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 James S. Stevenson . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: February 16, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>